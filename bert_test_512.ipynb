{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment_Id</th>\n",
       "      <th>Comment_Text</th>\n",
       "      <th>Racial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203468</td>\n",
       "      <td>Ok thanks you for your answer I am a beginner ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>225898</td>\n",
       "      <td>Standard says about arrow comparisons Otherwis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223540</td>\n",
       "      <td>Whoops I m on roving so seeing everything dela...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244364</td>\n",
       "      <td>FWIW this question ranks highly on Google when...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233839</td>\n",
       "      <td>Are you able to post a sample of the data to S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>133936</td>\n",
       "      <td>how come that its a spam you nigga?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>132338</td>\n",
       "      <td>English? This is english! Learn how to code idiot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>107627</td>\n",
       "      <td>Questions in English do not start with “But I ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>129968</td>\n",
       "      <td>You're not really answering to the essential q...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>128284</td>\n",
       "      <td>You mean, *\"there really needs to be an **all-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3470 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Comment_Id                                       Comment_Text  Racial\n",
       "0         203468  Ok thanks you for your answer I am a beginner ...       0\n",
       "1         225898  Standard says about arrow comparisons Otherwis...       0\n",
       "2         223540  Whoops I m on roving so seeing everything dela...       0\n",
       "3         244364  FWIW this question ranks highly on Google when...       0\n",
       "4         233839  Are you able to post a sample of the data to S...       0\n",
       "...          ...                                                ...     ...\n",
       "3465      133936                how come that its a spam you nigga?       1\n",
       "3466      132338  English? This is english! Learn how to code idiot       1\n",
       "3467      107627  Questions in English do not start with “But I ...       1\n",
       "3468      129968  You're not really answering to the essential q...       1\n",
       "3469      128284  You mean, *\"there really needs to be an **all-...       1\n",
       "\n",
       "[3470 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('bert_test.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Racial\n",
       "0    0.5\n",
       "1    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Racial'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #split train dataset into train, validation and test sets\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['Comment_Text'], df['Racial'],\n",
    "\n",
    "                                                                    random_state=2018,\n",
    "\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    \n",
    "                                                                    stratify=df['Racial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "\n",
    "                                                                random_state=2018,\n",
    "\n",
    "                                                                test_size=0.5,\n",
    "\n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# import BERT-base pretrained model\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "\n",
    "    train_text.tolist(),\n",
    "\n",
    "    max_length = None,#max_length = 25,\n",
    "\n",
    "    padding = True,#pad_to_max_length=True,\n",
    "\n",
    "    truncation=True\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "\n",
    "    val_text.tolist(),\n",
    "\n",
    "    max_length = None,#max_length = 25,\n",
    "\n",
    "    padding = True,#pad_to_max_length=True,\n",
    "\n",
    "    truncation=True\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "\n",
    "    test_text.tolist(),\n",
    "\n",
    "    max_length = None, #max_length = 25,\n",
    "\n",
    "    padding = True,#pad_to_max_length=True,\n",
    "\n",
    "    truncation=True\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "              # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\64220\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.00041186 0.99958848]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "# push to GPU\n",
    "\n",
    "#weights = weights.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# define the loss function\n",
    "\n",
    "cross_entropy  = nn.NLLLoss(weight=weights)\n",
    "\n",
    "\n",
    "\n",
    "# number of training epochs\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "\n",
    "def train():\n",
    "\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "\n",
    "\n",
    "    # empty list to save model predictions\n",
    "\n",
    "    total_preds=[]\n",
    "\n",
    "\n",
    "\n",
    "    # iterate over batches\n",
    "\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "# progress update after every 50 batches.\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "\n",
    "\n",
    "        # push the batch to gpu\n",
    "\n",
    "        #batch = [r.to(device) for r in batch]\n",
    "\n",
    "        batch = [r for r in batch]\n",
    "\n",
    "\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "\n",
    "\n",
    "        # clear previously calculated gradients\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "\n",
    "\n",
    "        # add on to the total loss\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "\n",
    "        # update parameters\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "    # append the model predictions\n",
    "\n",
    "    total_preds.append(preds)\n",
    "\n",
    "\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    #returns the loss and predictions\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "\n",
    "\n",
    "    # deactivate dropout layers\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "\n",
    "    total_preds = []\n",
    "\n",
    "\n",
    "\n",
    "    # iterate over batches\n",
    "\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "\n",
    "\n",
    "            # Report progress.\n",
    "\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "\n",
    "\n",
    "        # push the batch to gpu\n",
    "\n",
    "        #batch = [t.to(device) for t in batch]\n",
    "\n",
    "        batch = [t for t in batch]\n",
    "\n",
    "\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "\n",
    "\n",
    "        # deactivate autograd\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "\n",
    "\n",
    "            # model predictions\n",
    "\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.656\n",
      "Validation Loss: 0.617\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.609\n",
      "Validation Loss: 0.570\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.569\n",
      "Validation Loss: 0.553\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.555\n",
      "Validation Loss: 0.528\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.542\n",
      "Validation Loss: 0.506\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.518\n",
      "Validation Loss: 0.492\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.508\n",
      "Validation Loss: 0.480\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.490\n",
      "Validation Loss: 0.459\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.478\n",
      "Validation Loss: 0.440\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of     76.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.453\n",
      "Validation Loss: 0.420\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "\n",
    "train_losses=[]\n",
    "\n",
    "valid_losses=[]\n",
    "\n",
    "\n",
    "\n",
    "#for each epoch\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "\n",
    "\n",
    "    #train model\n",
    "\n",
    "    train_loss, _ = train()\n",
    "\n",
    "\n",
    "\n",
    "    #evaluate model\n",
    "\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "\n",
    "\n",
    "    #save the best model\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "\n",
    "\n",
    "    # append training and validation loss\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82       261\n",
      "           1       0.78      0.94      0.85       260\n",
      "\n",
      "    accuracy                           0.83       521\n",
      "   macro avg       0.85      0.84      0.83       521\n",
      "weighted avg       0.85      0.83      0.83       521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #load weights of best model\n",
    "\n",
    "path = 'saved_weights.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    " \n",
    "\n",
    "# get predictions for test data\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        preds = model(test_seq, test_mask)\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# model's performanceex\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "racism_in_seng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
